<!-- SecciÃ³n de Calidad de CÃ³digo eliminada: no implementada en este repositorio. -->
# ğŸ¤– Apex Digital

![Python](https://img.shields.io/badge/Python-3.11-blue?logo=python&logoColor=white)
![FastAPI](https://img.shields.io/badge/FastAPI-0.115.12-green?logo=fastapi&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-Containerized-2496ED?logo=docker&logoColor=white)
![MLflow](https://img.shields.io/badge/MLflow-2.9.2-00A6E0?logo=mlflow&logoColor=white)
![Scikit-learn](https://img.shields.io/badge/scikit--learn-1.4.2-F89939?logo=scikit-learn&logoColor=white)
![Ruff](https://img.shields.io/badge/Ruff-Linter-blue?logo=ruff&logoColor=white)

Este proyecto es una soluciÃ³n a la prueba tÃ©cnica de Data Science para Apex Digital. El objetivo es construir un pipeline de machine learning de extremo a extremo, desde el procesamiento de datos y entrenamiento de modelos hasta el despliegue de una API de inferencia con seguimiento de experimentos.

## ğŸ“‹ Tabla de Contenidos

- [ğŸš€ CaracterÃ­sticas Principales](#-caracterÃ­sticas-principales)
- [ğŸ—ï¸ Arquitectura del Pipeline](#-arquitectura-del-pipeline)
- [ğŸ“ Estructura del Proyecto](#-estructura-del-proyecto)
- [ğŸ› ï¸ Stack TecnolÃ³gico](#-stack-tecnolÃ³gico)
- [ğŸ“‹ Prerrequisitos](#-prerrequisitos)
- [âš™ï¸ InstalaciÃ³n y ConfiguraciÃ³n](#-instalaciÃ³n-y-configuraciÃ³n)
- [ğŸš€ Uso](#-uso)
- [ğŸ“Š Monitoreo y Logging](#-monitoreo-y-logging)
- [ğŸš¢ Despliegue](#-despliegue)
- [ğŸ¯ Enfoque](#-enfoque)
- [ğŸ§¾ Hallazgos](#-hallazgos)
- [ğŸ”§ Mejoras](#-mejoras)
- [ğŸ“ˆ Resultados de Inferencia](#-resultados-de-inferencia)
- [ğŸ¤ CÃ³mo Contribuir](#-cÃ³mo-contribuir)
- [ğŸ“„ Licencia](#-licencia)

## ğŸš€ CaracterÃ­sticas Principales

- **ğŸ“Š Pipeline ETL Completo**: Scripts para extracciÃ³n, transformaciÃ³n y carga de datos desde archivos Parquet.
- **ğŸ¤– Entrenamiento y SelecciÃ³n de Modelos**: Proceso automatizado para entrenar mÃºltiples modelos de clasificaciÃ³n y seleccionar el mejor basado en mÃ©tricas.
- **ğŸ” Seguimiento de Experimentos con MLflow**: Registro detallado de parÃ¡metros, mÃ©tricas y artefactos de cada ejecuciÃ³n.
- **âš¡ API de Inferencia con FastAPI**: Servicio web asÃ­ncrono para realizar predicciones en tiempo real.
- **ğŸ³ ContenerizaciÃ³n con Docker**: Despliegue reproducible y aislado de la API y la interfaz de MLflow usando Docker Compose.
- **ğŸ› ï¸ AutomatizaciÃ³n con Makefile**: Comandos simplificados para ejecutar el pipeline, gestionar servicios y realizar tareas comunes.
 - **âœ… AutomatizaciÃ³n con calidad de cÃ³digo (mejora pendiente)**: Actualmente no hay linters ni hooks configurados en el repositorio; ver la secciÃ³n "Limitaciones y Posibles Mejoras" para recomendaciones.



## ğŸ—ï¸ Arquitectura del Pipeline

El proyecto sigue un flujo de trabajo de MLOps estÃ¡ndar, desde los datos crudos hasta el servicio en producciÃ³n. A continuaciÃ³n se detalla el flujo y las responsabilidades principales de cada componente:

- **ETL (Extract, Transform, Load)** ğŸ“¥: Lee los archivos Parquet en `data/raw/`, aplica limpieza y transformaciones (gestiÃ³n de nulos, codificaciÃ³n de categÃ³ricas) y escribe el dataset final en `data/processed/data_final.parquet`.
- **Split** âœ‚ï¸: Divide el dataset procesado en conjuntos de entrenamiento y test (estratificado cuando aplica), manteniendo reproducibilidad con semillas controladas en `split.py`.
- **Training** ğŸ‹ï¸: Entrena una colecciÃ³n de modelos candidatos usando `training.py`, registra parÃ¡metros y mÃ©tricas en MLflow y guarda artefactos (modelos serializados) en el store configurado.
- **Selection** ğŸ†: Lee los resultados en MLflow y selecciona el mejor modelo segÃºn la mÃ©trica objetivo (por defecto F1 score). El modelo seleccionado se marca como `best` o se versiona segÃºn la estrategia configurada en `selection.py`.
- **Inference** ğŸ”®: Servicio ligero que carga el modelo seleccionado y expone una interfaz para recibir datos y devolver predicciones. Optimizado para baja latencia y registra tiempos de procesamiento por peticiÃ³n.
- **FastAPI** âš¡: Aplica validaciÃ³n de entrada (Pydantic), endpoints para inferencia en tiempo real y un endpoint de salud / mÃ©tricas. Contenerizado para despliegue.


```mermaid
graph TD
    subgraph "1. OrquestaciÃ³n Local"
        A[main.py] --> B{ETL};
        A --> C{Split};
        A --> D{Training};
        A --> E{Selection};
        A --> F{Inference};
    end

    subgraph "2. Procesamiento de Datos"
        B --> G[data/raw/*.parquet];
        G --> H[processors.py];
        H --> I[data/processed/data_final.parquet];
    end

    subgraph "3. Machine Learning"
        C --> J[split.py];
        J --> K[Datos de Entrenamiento/Test];
        D --> L[training.py];
        L -- Registra experimento --> M[MLflow Tracking Server];
        E --> N[selection.py];
        N -- Consulta experimentos --> M;
        N --> O[Mejor Modelo];
    end

    subgraph "4. Servidor de Modelos"
        F --> P[inference.py];
        P -- Carga modelo desde --> M;
        P --> Q[Resultados de Inferencia];
    end

    subgraph "5. Despliegue con Docker"
        R[docker-compose.yaml] --> S[Servicio FastAPI];
        R --> T[Servicio MLflow];
        S -- Usa modelo de --> T;
    end

    I --> J;
    K --> L;
    O --> P;
```

<!-- Diagrama de arquitectura eliminado: no se renderizaba correctamente en Markdown. -->

---

## ğŸ“ Estructura del Proyecto

```
Apex_Digital/
â”œâ”€â”€ config/                   # âš™ï¸ Archivos de configuraciÃ³n (logger, settings)
â”œâ”€â”€ data/                     # ğŸ“Š Datos crudos y procesados
â”œâ”€â”€ deployment/               # ğŸ³ Archivos para despliegue (Dockerfiles)
â”œâ”€â”€ docs/                     # ğŸ“š DocumentaciÃ³n adicional
â”œâ”€â”€ logs/                     # ğŸ“ Archivos de logging
â”œâ”€â”€ notebooks/                # ğŸ““ Jupyter notebooks para exploraciÃ³n (EDA)
â”œâ”€â”€ src/                      # ğŸ CÃ³digo fuente del proyecto
â”‚   â”œâ”€â”€ serving/              # âš¡ LÃ³gica de la API FastAPI
â”‚   â”œâ”€â”€ utils/                # ğŸ› ï¸ Funciones y clases de utilidad
â”‚   â”œâ”€â”€ elt.py                # ğŸ”„ Script para ETL
â”‚   â”œâ”€â”€ split.py              # ğŸ”ª Script para divisiÃ³n de datos
â”‚   â”œâ”€â”€ training.py           # ğŸ‹ï¸ Script para entrenamiento de modelos
â”‚   â”œâ”€â”€ selection.py          # ğŸ† Script para selecciÃ³n del mejor modelo
â”‚   â””â”€â”€ inference.py          # ğŸ”® Script para inferencia
â”œâ”€â”€ main.py                   # ğŸš€ Orquestador principal del pipeline
â”œâ”€â”€ Makefile                  # ğŸ› ï¸ Comandos de automatizaciÃ³n
â”œâ”€â”€ pyproject.toml            # ğŸ“¦ ConfiguraciÃ³n del proyecto y dependencias
â”œâ”€â”€ README.md                 # ğŸ“– Esta documentaciÃ³n
â””â”€â”€ ...
```

## ğŸ› ï¸ Stack TecnolÃ³gico

### Backend & Machine Learning
- **ğŸ Python 3.11**: Lenguaje principal del proyecto.
- **âš¡ FastAPI**: Framework web para construir la API de inferencia.
- **ğŸ¤– Scikit-learn**: LibrerÃ­a fundamental para los modelos de machine learning.
- **ğŸ“Š Pandas & PyArrow**: Para manipulaciÃ³n y procesamiento de datos eficiente.
- **ğŸ” MLflow**: Para el seguimiento de experimentos, versionado de modelos y despliegue.
- **Pydantic**: Para la validaciÃ³n de datos en la API.

### ContenerizaciÃ³n & DevOps
- **ğŸ³ Docker & Docker Compose**: Para crear, gestionar y orquestar los servicios en contenedores.
- **UV**: Instalador y gestor de paquetes Python ultra-rÃ¡pido.
- **Makefile**: Para automatizar tareas repetitivas de desarrollo y despliegue.


## ğŸ“‹ Prerrequisitos

AsegÃºrate de tener instalado lo siguiente en tu sistema:
- **Python 3.11+**
- **Docker & Docker Compose**
- **Git**
- **`uv`** (recomendado para la gestiÃ³n de dependencias): `pip install uv`

## âš™ï¸ InstalaciÃ³n y ConfiguraciÃ³n

1.  **Clonar el repositorio:**
    ```bash
    git clone <URL_DEL_REPOSITORIO>
    cd Apex_Digital
    ```

2.  **Crear y activar un entorno virtual:**
    Se recomienda usar `uv` por su velocidad.
    ```bash
    uv venv
    source .venv/bin/activate
    ```
    *(En Windows, usa: `.venv\Scripts\activate`)*

3.  **Instalar dependencias:**
    ```bash
    uv pip install -r requirements.txt
    ```

4.  **Notas sobre configuraciÃ³n y secretos (mejora recomendada):**
    Actualmente el repositorio no incluye un archivo de configuraciÃ³n para variables de entorno ni gestiÃ³n de secretos. Para producciÃ³n recomendamos usar un gestor de secretos (Azure Key Vault, HashiCorp Vault) y definir variables de entorno en la configuraciÃ³n del despliegue.

## ğŸš€ Uso

### EjecuciÃ³n del Pipeline Completo
Para ejecutar todos los pasos del pipeline (ETL, split, training, selection, inference) en secuencia:
```bash
make run
# O directamente:
python main.py
```

### EjecuciÃ³n con Docker (Recomendado)
Esto levantarÃ¡ la API de FastAPI y la interfaz de MLflow en contenedores.
```bash
make up
```
Servicios disponibles:
- **API de FastAPI**: `http://localhost:8008/docs`
- **UI de MLflow**: `http://localhost:5000`

### Comandos del Makefile
-   **`make run`**: Ejecuta el pipeline principal localmente.
-   **`make train`**: Ejecuta solo el paso de entrenamiento.
-   **`make mlflow-ui`**: Inicia la interfaz de usuario de MLflow localmente.
-   **`make up`**: Levanta los servicios de Docker (FastAPI y MLflow).
-   **`make down`**: Detiene y elimina los contenedores de Docker.
-   **`make logs`**: Muestra los logs de los servicios en ejecuciÃ³n.
 -   **`make format`**: (No implementado) Formateo automÃ¡tico recomendado.
 -   **`make lint`**: (No implementado) Linting recomendado.

## ğŸ“Š Monitoreo y Logging

### Logs de la AplicaciÃ³n
Los logs generados por la aplicaciÃ³n se guardan en `logs/app.log`. Puedes verlos en tiempo real:
```bash
tail -f logs/app.log
```

### Logs de Docker
Para monitorear los servicios que corren en Docker:
```bash
make logs
# Para un servicio especÃ­fico:
docker compose logs -f fastapi_service
docker compose logs -f mlflow_service
```

### Interfaz de MLflow
Accede a `http://localhost:5000` para ver y comparar todos los experimentos, mÃ©tricas y modelos entrenados.

<!-- SecciÃ³n de Calidad de CÃ³digo eliminada: no implementada en este repositorio. -->

## ğŸš¢ Despliegue

El despliegue se gestiona a travÃ©s de Docker, lo que garantiza un entorno consistente.

1.  **Construir las imÃ¡genes de Docker:**
    ```bash
    docker compose build
    ```

2.  **Levantar los servicios:**
    ```bash
    make up
    ```

Los archivos `Dockerfile` para cada servicio se encuentran en la carpeta `deployment/`:
-   `deployment/fastapi/Dockerfile`: Para la API de inferencia.
-   `deployment/mlflow/Dockerfile`: Para el servicio de MLflow.

El archivo `docker-compose.yaml` orquesta el despliegue de estos servicios.

## ğŸ¯ Enfoque

El proyecto sigue un enfoque MLOps implementado y orquestado por `main.py`. A continuaciÃ³n se describen, de forma concisa y responsable, las responsabilidades y pasos que el cÃ³digo ejecuta:

- **MLflow & Experimentos**: `main.py` configura `MLFLOW_TRACKING_URI`, asegura el experimento (`ensure_experiment`) y abre un `mlflow.start_run` (`async_inference_run`) para centralizar el logging de parÃ¡metros, mÃ©tricas y artefactos.
- **ELT (ExtracciÃ³n, Limpieza y Carga)**: Se usa `ELTPipeline` (`src.elt`) para leer `data/raw/`, aplicar las transformaciones necesarias (p. ej. `dtype_map`) y escribir el dataset procesado en `data/processed/`.
- **Split reproducible**: DivisiÃ³n `train/val/test` mediante `split_train_val_test` con semilla fija para reproducibilidad; el target principal es `canal_pedido_cd`.
- **SelecciÃ³n y alineado de features**: `feature_selection` garantiza que `X_train`, `X_val`, `X_test` compartan el mismo conjunto y orden de columnas antes de entrenar.
- **Entrenamiento y registro de modelos (XGBoost)**: El pipeline de entrenamiento (`train_models`) entrena un **XGBoost (XGBClassifier)** como modelo principal, con opciones aplicadas desde `main.py` (por ejemplo `use_balanced_weights`, `use_gpu`, `early_stopping_rounds`). Los runs registran mÃ©tricas y artefactos en MLflow y el modelo queda asegurado en el Model Registry (nombre por defecto `digital_orders_xgboost`).
- **Garantizar modelo en Registry**: La funciÃ³n `ensure_model_available` verifica que exista al menos una versiÃ³n registrada; si no, dispara el entrenamiento (`train_models`) y vuelve a validar.
- **Inferencia asÃ­ncrona y pruebas**: Se prepara una muestra de test (`prepare_test_sample`) y se lanza `run_async_inference` con concurrencia (`max_concurrent`) para medir latencia, tasa de Ã©xito y confianza. El flujo es asÃ­ncrono para simular cargas reales y medir tiempos de procesamiento.
- **MÃ©tricas y artefactos de inferencia**: El cÃ³digo agrega mÃ©tricas agregadas a MLflow (`inference_samples`, `inference_total_time_ms`, `inference_avg_time_ms`, `inference_success_rate`), logs por-clase (`pred_count_<class>`), estadÃ­sticas de confianza y guarda archivos JSON (`inference_results.json`, `inference_distributions.json`) como artefactos.
- **Alineado y logging legible**: Internamente se usa una utilidad para alinear la distribuciÃ³n de `ground_truth` con la de predicciones (`_align_dict_like`) para reporting claro en logs y artefactos.
- **OrquestaciÃ³n end-to-end**: `main.py` actÃºa como orquestador Ãºnico (ELT â†’ split â†’ feature selection â†’ entrenamiento/registro â†’ inferencia â†’ logging), permitiendo ejecutar el pipeline completo con reproducibilidad y trazabilidad.

## ğŸ§¾ Hallazgos

- **Resumen del rendimiento del modelo:** val_logloss observado â‰ˆ 1.04862 (mÃ©tricas de validaciÃ³n registradas durante entrenamiento en `src/training.py` y replicadas en `logs/app.log`).
- **TamaÃ±o y preparaciÃ³n de datos:** ELT produjo un dataset concatenado de 1,250,000 registros que se redujo a 149,960 clientes Ãºnicos; promedio ~8.3 registros/cliente; archivo procesado guardado en `data/processed/data_final.parquet`.
- **Split de datos:** Train = 89,976 / Val = 29,992 / Test = 29,992 (ver entradas de `logs/app.log`).
- **Features finales usadas:** 8 features finales tras selecciÃ³n automÃ¡tica: `['pais_cd', 'tipo_cliente_cd', 'madurez_digital_cd', 'estrellas_txt', 'frecuencia_visitas_cd', 'cajas_fisicas', 'fecha_pedido_dt_day_of_week', 'fecha_pedido_dt_quarter']` (ver `src/selection/feature_selection.py` y logs).
- **Comportamiento en inferencia (resumen cuantitativo):**
    - Muestras procesadas en tests end-to-end: 50 por corrida (varias ejecuciones).
    - Latencia: promedio por predicciÃ³n entre ~14.5ms y ~16.7ms (ejecuciones en logs); ejemplo agregado: promedio global â‰ˆ 15ms/predicciÃ³n.
    - Confianza promedio por ejecuciÃ³n: observada entre ~0.486 y ~0.547; valor medio de ejemplos leÃ­dos â‰ˆ 0.51.
- **DistribuciÃ³n de clases â€” sesgos observados:**
    - Predicciones (ejemplo agregado): `DIGITAL` frecuentemente sobre-predicho respecto al ground truth; `VENDEDOR` tiende a estar sub-predicho.
    - Ejemplo desde `inference_distributions.json`: `predictions = {'TELEFONO': 17, 'DIGITAL': 28, 'VENDEDOR': 5}` vs `ground_truth_aligned = {'TELEFONO': 10, 'DIGITAL': 24, 'VENDEDOR': 16}` â€” sugiere sobre-predicciÃ³n de `DIGITAL` y dÃ©ficit en `VENDEDOR`.
- **Ã‰xito operacional:** Inferencias batch muestran success rate 100% en runs observadas; modelo cargado desde registry `models:/digital_orders_xgboost/2` (logs de `src/inference/initialize`).
- **Limitaciones detectadas (evidencia):**
    - MÃ©trica de validaciÃ³n (logloss) relativamente alta â†’ modelo puede no estar listo para producciÃ³n sin calibraciÃ³n adicional o features mÃ¡s informativas.
    - Desbalance en predicciones vs ground truth por clase â€” riesgo de sesgo que requiere revisiÃ³n de clases, weights o muestreo.
    - Feature selection eliminÃ³ columnas potencialmente informativas (`facturacion_usd_val`, `materiales_distintos_val`) por correlaciÃ³n baja o alta multicolinealidad â€” revisar con input de negocio.
- **Acciones recomendadas (corto/medio plazo):**
    - Re-evaluar balance de clases (re-muestreo o `class_weight`) y/o ajustar thresholds por clase.
    - Calibrar probabilidades (Platt scaling / isotonic) si se requiere confianza probabilÃ­stica robusta.
    - Revisar variables eliminadas por selecciÃ³n automÃ¡tica para confirmar si su eliminaciÃ³n es apropiada desde el negocio.
    - AÃ±adir mÃ©tricas de clasificaciÃ³n por clase (precision/recall/F1, matriz de confusiÃ³n) en el pipeline de evaluaciÃ³n y en los artifacts de MLflow.
- **DÃ³nde encontrar la evidencia:**
    - Logs: `logs/app.log` (ejecuciones que muestran ELT, split, feature selection, inferencia y mÃ©tricas).
    - Artefactos de inferencia: `inference_results.json`, `inference_distributions.json` (muestras de predicciones, confidencias y latencias).
    - CÃ³digo: `src/training.py`, `src/selection/feature_selection.py`, `src/inference/*`, `main.py`.
- **Diagrama de arquitectura:**
    - Archivo: `docs/architecture.png` (convertido desde SVG para compatibilidad con renderizado).

## ğŸ”§ Mejoras

- **Rendimiento del Modelo**: El modelo actual (XGBoost) es un buen punto de partida. Siguientes pasos recomendados:
    - **IngenierÃ­a de CaracterÃ­sticas Avanzada**: Crear nuevas variables que capturen mejor las relaciones en los datos.
    - **Explorar alternativas y ensamblados**: Probar LightGBM, CatBoost o ensamblados (stacking/blending) que pueden complementar XGBoost.
    - **OptimizaciÃ³n de HiperparÃ¡metros**: Realizar una bÃºsqueda mÃ¡s exhaustiva (ej. con Optuna o Hyperopt, integrados con MLflow) y usar tÃ©cnicas como cross-validation mÃ¡s robustas.
- **Falta de Pruebas (Testing)**: El proyecto carece de una suite de tests unitarios y de integraciÃ³n. AÃ±adir pruebas es **crÃ­tico** para asegurar la fiabilidad del cÃ³digo, validar los datos y prevenir regresiones.
- **Seguridad**: El cÃ³digo actual no gestiona secretos de forma segura. En un entorno productivo, las credenciales y claves de API deberÃ­an ser gestionadas a travÃ©s de un sistema como Azure Key Vault o HashiCorp Vault, en lugar de estar expuestas.
- **AutomatizaciÃ³n (CI/CD)**: El pipeline se ejecuta manualmente. El siguiente paso serÃ­a implementar un sistema de IntegraciÃ³n y Despliegue Continuo (CI/CD) con herramientas como GitHub Actions para automatizar la ejecuciÃ³n del pipeline ante nuevos commits.
- **Monitoreo en ProducciÃ³n**: La monitorizaciÃ³n actual se limita a logs. Un sistema productivo requerirÃ­a un monitoreo activo del modelo para detectar **deriva de datos (data drift)** y **degradaciÃ³n del rendimiento del modelo (model degradation)**.

## ğŸ“ˆ Resultados de Inferencia

- Total predicciones: **50**
- Tiempo medio de procesamiento por peticiÃ³n: **15.03 ms**
- Tiempo mediano de procesamiento: **14.79 ms**
- Confianza media: **0.511**
- Confianza mediana: **0.489**

- Conteo por clase:
    - **TELEFONO**: 17
    - **DIGITAL**: 28
    - **VENDEDOR**: 5

- Confianza media por clase:
    - **TELEFONO**: 0.4022
    - **DIGITAL**: 0.5993
    - **VENDEDOR**: 0.3854

- Clase mÃ¡s frecuente: **DIGITAL** (28 predicciones)

## ğŸ¤ CÃ³mo Contribuir

Las contribuciones son bienvenidas. Por favor, sigue estos pasos:
1.  Haz un **Fork** de este repositorio.
2.  Crea una nueva **rama** (`git checkout -b feature/nueva-funcionalidad`).
3.  Realiza tus cambios y haz **commit** (`git commit -m 'AÃ±adir nueva funcionalidad'`).
4.  AsegÃºrate de que el cÃ³digo pasa las verificaciones de calidad (`make format` y `make lint`).
5.  Haz **Push** a tu rama (`git push origin feature/nueva-funcionalidad`).
6.  Abre un **Pull Request**.

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Consulta el archivo `LICENSE` para mÃ¡s detalles.
