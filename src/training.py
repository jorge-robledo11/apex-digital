"""
Orquestaci√≥n del entrenamiento con XGBoost + MLflow.

- Convierte features categ√≥ricas a dtype 'category' (v√≠a utils).
- Codifica SOLO el target a c√≥digos num√©ricos (v√≠a utils).
- Entrena con early stopping y GPU con fallback a CPU.
- Calcula y empaqueta m√©tricas con modelos Pydantic (src/utils/metrics.py).
- Loguea par√°metros, m√©tricas y artefactos en MLflow.
"""

from typing import Any

import mlflow
import mlflow.xgboost
import pandas as pd
from mlflow.models import infer_signature
from xgboost import XGBClassifier

from config.settings import get_settings
from src.utils.training_utils import (
    prepare_categorical_features,
    encode_target_like_train,
    compute_balanced_sample_weight,
    predict_with_best,
    predict_proba_with_best,
    make_xgb_params,
)
from src.utils.metrics import (
    DatasetShapeInfo,
    TrainingFlags,
    XGBHyperparams,
    BestIterationInfo,
    TrainingMetrics,
    compute_classification_metrics,
    log_training_metrics_to_mlflow,
)

settings = get_settings()
logger = settings.logger


def _safe_log_params(params: dict[str, Any]) -> None:
    """Loguea par√°metros en MLflow sin romper el flujo si ocurre un error."""
    try:
        mlflow.log_params(params)
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è No se pudieron loguear algunos par√°metros: {e}")


def train_models(
    X_train: pd.DataFrame,
    X_val: pd.DataFrame,
    X_test: pd.DataFrame,
    y_train: pd.Series,
    y_val: pd.Series,
    y_test: pd.Series,
    use_balanced_weights: bool = True,
    use_gpu: bool = True,
    early_stopping_rounds: int = 50,
) -> tuple[XGBClassifier, float, str]:
    """
    Entrena un `XGBClassifier` multiclase y registra el proceso en MLflow.

    Aplica:
      - Features categ√≥ricas nativas (enable_categorical=True).
      - Codificaci√≥n del target (c√≥digos num√©ricos).
      - Balanceo opcional de clases mediante sample_weight.
      - Early stopping y fallback autom√°tico a CPU si GPU falla.
      - C√°lculo y empaquetado de m√©tricas con modelos Pydantic.
      - Registro de m√©tricas/artefactos en MLflow.

    Args:
        X_train: Matriz de features de entrenamiento.
        X_val: Matriz de features de validaci√≥n.
        X_test: Matriz de features de prueba.
        y_train: Target de entrenamiento.
        y_val: Target de validaci√≥n.
        y_test: Target de prueba.
        use_balanced_weights: Si se aplican pesos balanceados por clase.
        use_gpu: Si se intenta entrenar sobre GPU.
        early_stopping_rounds: Rondas m√°ximas sin mejora para detener.

    Returns:
        tuple[XGBClassifier, float, str]: (modelo_entrenado, val_logloss, "XGBoost").

    Raises:
        ValueError: Si alg√∫n split requerido est√° vac√≠o o el target tiene clases no vistas.
        RuntimeError: Si el entrenamiento falla en GPU y CPU, o si el c√°lculo de m√©tricas no es consistente.
    """
    logger.info("üöÄ Iniciando entrenamiento con XGBoost...")

    # -------------------------
    # Validaciones b√°sicas
    # -------------------------
    if X_train.empty or y_train.empty:
        logger.error("‚ùå Datos de entrenamiento vac√≠os.")
        raise ValueError("X_train/y_train no pueden estar vac√≠os.")
    if X_val.empty or y_val.empty:
        logger.error("‚ùå Datos de validaci√≥n vac√≠os.")
        raise ValueError("X_val/y_val no pueden estar vac√≠os.")
    if X_test.empty or y_test.empty:
        logger.error("‚ùå Datos de prueba vac√≠os.")
        raise ValueError("X_test/y_test no pueden estar vac√≠os.")

    # -------------------------
    # 1) Features categ√≥ricas
    # -------------------------
    cat_cols = prepare_categorical_features(X_train, X_val, X_test)

    # -------------------------
    # 2) Codificaci√≥n del target
    # -------------------------
    try:
        y_train_enc, y_val_enc, y_test_enc, class_names = encode_target_like_train(
            y_train, y_val, y_test
        )
    except ValueError as e:
        logger.error(f"‚ùå Error al codificar target: {e}")
        raise

    # -------------------------
    # 3) Par√°metros del modelo
    # -------------------------
    params = make_xgb_params(use_gpu=use_gpu, early_stopping_rounds=early_stopping_rounds)
    if use_gpu:
        logger.info("üü¢ Se intentar√° entrenar con GPU (gpu_hist).")
    model = XGBClassifier(**params)

    # -------------------------
    # 4) sample_weight (opcional)
    # -------------------------
    fit_kwargs: dict[str, Any] = {}
    if use_balanced_weights:
        try:
            sw = compute_balanced_sample_weight(y_train_enc)
            fit_kwargs["sample_weight"] = sw
            logger.info("‚öñÔ∏è  Usando sample_weight balanceado por clase.")
        except ValueError as e:
            logger.warning(f"‚ö†Ô∏è No se pudo calcular sample_weight balanceado: {e}")

    # =========================================================
    # Run anidado (cuelga del run activo en el proceso padre)
    # =========================================================
    with mlflow.start_run(run_name="xgboost_training", nested=True):
        logger.info("üîó Run de entrenamiento anidado al run activo de MLflow.")
        logger.info("üìä Entrenando XGBoost con early stopping‚Ä¶")

        # Log de tama√±os y flags informativos
        _safe_log_params(
            {
                "n_train": len(X_train),
                "n_val": len(X_val),
                "n_test": len(X_test),
                "n_features": X_train.shape[1],
                "early_stopping_rounds": early_stopping_rounds,
                "used_balanced_weights": use_balanced_weights,
                "used_gpu": use_gpu,
                "categorical_features": True,
                "encoded_target": True,
            }
        )
        if cat_cols:
            try:
                mlflow.log_text("\n".join(map(str, cat_cols)), "artifacts/categorical_features.txt")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è No se pudieron loguear las columnas categ√≥ricas: {e}")
        try:
            mlflow.log_text("\n".join(map(str, class_names)), "artifacts/class_names.txt")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è No se pudieron loguear los nombres de clase: {e}")

        # -------------------------
        # 5) Entrenamiento (GPU ‚Üí CPU fallback)
        # -------------------------
        try:
            model.fit(
                X_train,
                y_train_enc,
                eval_set=[(X_val, y_val_enc)],
                verbose=False,
                **fit_kwargs,
            )
        except Exception as e:
            if use_gpu:
                logger.warning(f"‚ö†Ô∏è Entrenamiento con GPU fall√≥ ({e}). Reintentando en CPU‚Ä¶")
                params = make_xgb_params(use_gpu=False, early_stopping_rounds=early_stopping_rounds)
                model = XGBClassifier(**params)
                try:
                    model.fit(
                        X_train,
                        y_train_enc,
                        eval_set=[(X_val, y_val_enc)],
                        verbose=False,
                        **fit_kwargs,
                    )
                except Exception as e2:
                    logger.error(f"‚ùå Reintento en CPU fall√≥: {e2}")
                    raise RuntimeError("Fall√≥ el entrenamiento tanto en GPU como en CPU.") from e2
                logger.info("‚úÖ Reintento en CPU exitoso.")
            else:
                logger.error(f"‚ùå Entrenamiento en CPU fall√≥: {e}")
                raise RuntimeError("Fall√≥ el entrenamiento en CPU.") from e

        # -------------------------
        # 6) Predicciones (mejor iteraci√≥n)
        # -------------------------
        y_train_proba = predict_proba_with_best(model, X_train)
        y_val_proba = predict_proba_with_best(model, X_val)
        y_test_proba = predict_proba_with_best(model, X_test)

        y_train_pred = predict_with_best(model, X_train)
        y_val_pred = predict_with_best(model, X_val)
        y_test_pred = predict_with_best(model, X_test)

        # -------------------------
        # 7) M√©tricas (bundle Pydantic)
        # -------------------------
        try:
            losses, accuracies, reports, confusion = compute_classification_metrics(
                y_train_enc=y_train_enc,
                y_val_enc=y_val_enc,
                y_test_enc=y_test_enc,
                y_train_pred=y_train_pred,
                y_val_pred=y_val_pred,
                y_test_pred=y_test_pred,
                y_train_proba=y_train_proba,
                y_val_proba=y_val_proba,
                y_test_proba=y_test_proba,
                class_names=class_names,
            )
        except ValueError as e:
            logger.error(f"‚ùå Error en c√°lculo de m√©tricas: {e}")
            raise RuntimeError("C√°lculo de m√©tricas inconsistente.") from e

        shapes = DatasetShapeInfo(
            n_train=len(X_train), n_val=len(X_val), n_test=len(X_test), n_features=X_train.shape[1]
        )
        flags = TrainingFlags(
            early_stopping_rounds=early_stopping_rounds,
            used_balanced_weights=use_balanced_weights,
            used_gpu=use_gpu,
        )
        xgb_hparams = XGBHyperparams(
            objective=model.objective,
            n_estimators=model.n_estimators,
            learning_rate=model.learning_rate,
            max_depth=model.max_depth,
            min_child_weight=model.min_child_weight,
            subsample=model.subsample,
            colsample_bytree=model.colsample_bytree,
            reg_lambda=model.reg_lambda,
            reg_alpha=model.reg_alpha,
            tree_method=getattr(model, "tree_method", "unknown"),
            predictor=getattr(model, "predictor", "unknown"),
        )
        best_iter_info = BestIterationInfo(
            best_iteration=getattr(model, "best_iteration", None),
            best_val_mlogloss=(
                float(getattr(model, "best_score"))
                if getattr(model, "best_score", None) is not None
                else None
            ),
        )

        bundle = TrainingMetrics(
            shapes=shapes,
            flags=flags,
            losses=losses,
            accuracies=accuracies,
            reports=reports,
            confusion=confusion,
            xgb_hyperparams=xgb_hparams,
            best_iter_info=best_iter_info,
        )

        # -------------------------
        # 8) Logging consolidado a MLflow
        # -------------------------
        try:
            log_training_metrics_to_mlflow(bundle, mlflow_module=mlflow)
        except Exception as e:  # pragma: no cover
            logger.warning(f"‚ö†Ô∏è No se pudieron loguear m√©tricas en MLflow: {e}")

        # -------------------------
        # 9) Firma + registro del modelo
        # -------------------------
        try:
            signature = infer_signature(X_val, model.predict_proba(X_val))
        except Exception as e:  # pragma: no cover
            logger.warning(f"‚ö†Ô∏è No se pudo inferir la firma del modelo: {e}")
            signature = None

        try:
            mlflow.xgboost.log_model(
                model,
                artifact_path="xgboost_model",
                registered_model_name="digital_orders_xgboost",
                signature=signature,
            )
        except Exception as e:  # pragma: no cover
            logger.warning(f"‚ö†Ô∏è No se pudo loggear/registrar el modelo: {e}")

        # -------------------------
        # 10) Logs de cortes√≠a
        # -------------------------
        logger.info("üìà M√©tricas de Rendimiento:")
        logger.info(
            f"   üéØ LogLoss ‚Üí Train {losses.train_logloss:.4f} | "
            f"Val {losses.val_logloss:.4f} | Test {losses.test_logloss:.4f}"
        )
        logger.info(
            f"   üìä Acc    ‚Üí Train {accuracies.train_accuracy:.4f} | "
            f"Val {accuracies.val_accuracy:.4f} | Test {accuracies.test_accuracy:.4f}"
        )
        if best_iter_info.best_iteration is not None and best_iter_info.best_val_mlogloss is not None:
            logger.info(
                f"üèÅ Best iteration: {best_iter_info.best_iteration}  |  "
                f"Best val mlogloss: {best_iter_info.best_val_mlogloss:.6f}"
            )

    # Devolvemos el modelo y la m√©trica principal de validaci√≥n
    return model, float(losses.val_logloss), "XGBoost"
